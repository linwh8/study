# 线性代数

[TOC]

## 0 写在前面

写这个文档的初衷只是作为 [ecr23](http://ecr23.me/) 查漏补缺，且这个文档的终极目标并不是教材，所以我尽量避免了专业的表述而用了很多便于理解的表述，所以不够详尽和不够专业的地方请多多担待。欢迎大家补充。

## 1 基本运算

### 1.1 转置 transpose

以对角线为轴的镜像，从左上角到右下角的**主对角线**。标量的转置等于它本身

![](https://s1.ax1x.com/2018/05/31/C5qluD.png)

### 1.2 广播 broadcasting

当两个大小不是完全相等的矩阵相加的时候，如 $C = A+b$，如果 $A$ 的列数等于 $b$ 的列数，那么 $C$ 等于 $A$ 每行与 b 相加

### 1.3 矩阵乘法 multiplication

写作 $C=AB$，要求 $A$ 的列数等于 $B$ 的行数

$$
C_{i,j} = \sum_kA_{i,k}B_{k,j}
$$

### 1.4 向量点乘 dot product

写作 $c = x · y$，可以看作是矩阵乘法的 $x^Ty$，要求 $x$ 和 $y$ 长度一致（都是列向量）

$$
c = \sum_i x_i * y_i = |x| * |y| * cos\theta
$$

> *Note 1.4*
>
> 注意点乘满足交换律，即 $x · y = y · x$。但是矩阵乘积不可以。

### 1.5 元素对应乘积 element-wise product

写作 $C = A \circ B$，要求矩阵大小一致

$$
C_{ij} = A_{ij}*B_{ij}
$$

## 2 向量空间

### 2.1 线性组合 linear combination

通过线性方式（加/乘）组合不同的向量形成的向量就是**线性组合**。其形式如下

$$
Ax = \sum_i c_i v^{(i)}
$$

> *Example 2.1*
>
> 向量 $1*(1,2)^T + 2*(-1,3)^T = (-1, 8)^T$，那么 $(-1, 8)^T$ 就是 $(1,2)^T$ 和 $(-1,3)^T$ 的线性组合

### 2,2 生成子空间 span

**一组**向量通过线性组合组成的所有向量的集合就是生成**子空间**

> *Example 2.2*
>
> 在 *Example 2.1* 中，$(1,2)^T$ 和 $(-1,3)^T$ 的生成子空间就是 $R^2$，因为他们可以通过线性组合得到 $R^2$ 所有的向量。

> *Note 2.2*
>
> 子空间的应用范围是一组向量而非一个向量

### 2.3 列空间 column space / 值域 range

矩阵 $A$ 的每一列形成的子空间就是矩阵 $A$ 的**列空间**。因为方程 $Ax = b$ 的解 $x$ 就是对 $A$ 的列向量组的线性组合的权重 $c$（见 2.1 线性组合的公式）。所以列空间也可以理解成 $A$ 的列向量组通过线性组合能够得到的所有向量集合。

叫做值域是因为，这个向量集合就是矩阵函数 $f(x) = Ax$ 的值域

> *Note 2.3*
>
> 列空间的应用范围是矩阵 

### 2.4 线性相关 linear dependence

如果要使 $Ax = b$ 有解，需要 $A$ 的值域包含 $b$ 的取值范围。如果 $ b \in R^m$，则需要 $A$ 的值域包含 $R^m$，这需要 $A$ **至少**有 $m$ 列。因为要线性组合成 $R^m$ 至少需要 $m$ 个列向量。但是这个并不是一个充分条件，因为这些向量中可能有一些可以用另一些向量线性组合得到，这样这些向量就是被称作**线性相关**的。如果一组向量任意一个向量都不能表示成其他向量的线性组合，那么这组向量**线性无关**

> *Example 2.4*
>
> $(1,2)^T$ 和 $(2,4)^T$ 就是线性相关的一组向量，而 $(1,2)^T$ 和 $(-1,3)^T$ 就是线性无关 

## 3 矩阵方程 Ax = b

令 $A$ 有 $m$ 行 $n$ 列，$b$ 是一个长度为 $n$ 的列向量。矩阵 $A$ 的意义就在于从 $m$ 维空间映射到了 $n$ 维空间，$R^m \Rightarrow R^n$。由于 $A$ 有 $m$ 个 $n$ 维向量，这个方程也可以看作是寻找这 $m$ 个向量的线性组合到达 $b$ 

### 3.1 奇异矩阵 singular matrix

如果一个**方阵**列向量线性相关，这个矩阵就是**奇异矩阵**，否则就不是奇异矩阵

> *Note 3.1*
>
> 奇异矩阵必须是方阵

### 3.2 逆矩阵 inverse matrix

矩阵的逆矩阵记作 $A^{-1}$，满足 $A^{-1}A = AA^{-1} = I_n$，其中 $I_n$ 是单位矩阵。通过求一个矩阵的逆矩阵 $A^{-1}$ 我们可以求解方程 $Ax = b$
$$\begin{align}Ax &= b \\\\ A^{-1}Ax &= A^{-1}b\\\\x &= A^{-1}b\end{align}$$

> *Note 3.2*
>
> 矩阵并不是总是有逆矩阵，首先需要这个矩阵是一个**方**阵，然后还需要矩阵列向量线性无关。如果矩阵不是方阵就肯定没有逆矩阵。
>
> 或者可以理解成，因为有了逆矩阵，方程就有唯一的解 $A^{-1}b$。对于 $b \in R^m$ 的情况
>
> - $Ax=b$ 有解的**充要条件**是，$A$ 至少有 $m$ 个线性无关的列向量，即 $n \geq m$
> - $Ax=b$ 有**唯一解**的**充要条件是**，$A$ 恰好有 $m$ 个线性无关的列向量，即 $n=m$。因为如果有多于 $m$ 个线性无关的列向量，矩阵 $A$ 的列空间**包含**了 $R^m$，比如说用三个二维向量来组合得到一个二维向量，其实是有无数种组合方式的。所以逆矩阵必须要是方阵

## 4 分解

> Many mathematical objects can be understood better by breaking them into constituent parts, or finding some properties of them that are universal, not caused by the way we choose to represent them.
>
> from *Deep Learning*

摘抄了一段来自于《深度学习》的一段话。中文意思是，数学对象可以进行一定形式的分解来辅助我们的理解，比如一个数字可以对他进行质因子分解。矩阵也可以进行一些分解来帮助我们理解它们

### 4.1 特征分解 eigen-decomposition

对于下面的式子，标量 $\lambda$ 就是**方阵** $A$ 的**特征值**，$v$ 就是 $A$ 的**特征向量**

$$Av = \lambda v$$

直观的理解，$A$ 的特征向量就是左乘 $A$ 后被映射到一个缩放版本的自己上的向量。这个向量可以理解成矩阵 $A$ 的一个特征（因为几乎所有向量左乘 $A$ 之后都会改变方向，除了特征向量）。在下图中，假设矩阵 $A$ 有两个单位正交的特征向量 $v$，可以看到在左乘 $A$ 之后，这两个特征向量被拉长了，但是没有被改变方向。在这个例子中，其他向量可以写成两个特征向量的线性组合

![](https://i.loli.net/2018/05/31/5b0f641511e32.png)

> *Note 4.1.1*
> 特征向量可以无限缩放而不改变特征值，所以特征向量我们通常取单位特征向量。这是由于 $\lambda$ 是一个标量，所以定义式可以改写成
> $$\begin{align} sAv &= s\lambda v \\\\ A(sv) &= \lambda (sv) \\\\ \end{align}$$




假设矩阵 $A$ 有 $n$ 个**线性无关**的特征向量 $\{v^{(1)}, ..., v^{(n)}\}$，对应特征值 $\{\lambda_1, ..., \lambda_n\}$，我们可以把这些特征向量连接成一个矩阵 $V = [v^{(1)}, ..., v^{(n)}]$，特征值可以组合成一个对角矩阵 $\Lambda = [\lambda_1, ..., \lambda_n]^T$，$A$ 的**特征分解**可以记作

$$
A = V\Lambda V^{-1}
$$

> *Note 4.1.2*
>
> 不是每一个方阵都可以特征分解



回到特征向量定义的式子

$$\begin{align}Ax&=\lambda x\\\\(A-\lambda I)x &= 0\end{align}$$

所以要找特征向量，求解方程 $\det(A-\lambda I)x=0$ 即可

> *Reference 4.1*
>
> [Introduction to eigenvalues by MIT](math.mit.edu/linearalgebra/ila0601.pdf)

### 4.2 奇异值分解 singular value decomposition, SVD

除了特征分解还有另一种分解叫做奇异值分解 SVD，将矩阵分解成奇异值和奇异向量。因为特征分解要求比较高，不是方阵的矩阵不可以特征分解，就算是方阵也不一定有足够的特征向量来进行特征凤姐，所以奇异值分解应用更广

思路很简单，如果 $A$ 不是方阵就不能分解了，我们就构造一个方阵出来。构造方法就是 $AA^T$ 或者 $A^TA$。这样得到的矩阵就一定是方阵，而且还是对称的。通过计算 $AA^T$ 的特征值 $\sigma^2$ 和特征向量 $u$，$A^TA$ 的特征值  $\sigma^2$ 和特征向量 $u$，我们就可以得到 $A$ 的奇异值分解

$$
A = U\Sigma V^{T}=u_1\sigma_1v_1^T+...+u_r\sigma_rv_r^T
$$

为什么会这样？下面给一个解释

$$
A^TA = ( U\Sigma V^{T})^T( U\Sigma V^{T})=V\Sigma^TU^TU\Sigma V^T = V\Sigma^T\Sigma V^T
$$

所以 $A^TA$ 的特征向量就是 $A$ 的左奇异向量，特征值平方根就是 $A$ 的奇异值。$AA^T$ 类似。
